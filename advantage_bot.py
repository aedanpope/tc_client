# A bot which earns reward for maintaining an advantage, not just getting one.
# Only works for 1v1s


import tc_client
import argparse
import sys
import numpy as np
import tensorflow as tf
import random
import itertools
import sys
import math


MOVES = [(6,0), (-6,0), (0,6), (0,-6), (4,4), (4,-4), (-4,4), (-4,-4)]
# MOVES = [(10,0), (-10,0), (0,10), (0,-10), (7,7), (7,-7), (-7,7), (-7,-7)]
# friendly starts at (70,140), enemy starts at (100,140)
X_MOVE_RANGE = (40,140) # X_MOVE_RANGE and Y_MOVE_RANGE should be the same magnitude.
Y_MOVE_RANGE = (90,210)
FRIENDLY_TENSOR_SIZE = 7
ENEMY_TENSOR_SIZE = 3
MAX_FRIENDLY_UNITS = 1 # 5 for marines
MAX_ENEMY_UNITS = 1 # 5 for marines
EXTRA = 2
INP_SHAPE = MAX_FRIENDLY_UNITS * FRIENDLY_TENSOR_SIZE + MAX_ENEMY_UNITS * ENEMY_TENSOR_SIZE + EXTRA
OUT_SHAPE = 9 + MAX_ENEMY_UNITS
V = False  # Verbose


# Learning and env params:

# Exploration: probability of taking a random action instead of that suggested by the q-network.
INITIAL_EXPLORE = 0.9
# How much we multiply the current explore value by each cycle.
EXPLORE_FACTOR = 0.9995
# Dicount-rate: how much we depreciate future rewards in value for each state step.
# GAMMA = 0.99
GAMMA = 0.9
# Initial weights on neuron connections, need to start small so max output of NN isn't >> reward.
W_INIT = 0.01
# Reward val
REWARD = 1
MINI_REWARD = 0.1
MICRO_REWARD = 0.01
FRAMES_PER_ACTION = 1


## Generally, try and keep everything in the [-1,1] range.
## TODO: consider trying [0,1] range for some stuff e.g. HP.

def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)


def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)


class Battle:
  stages = None
  is_end = None
  is_won = None

  def __init__(self):
    self.stages = []
    self.is_end = False


  def add_stage(self, stage):
    self.stages.append(stage)
    if stage.is_end:
      self.is_end = True
      self.is_won = stage.is_won

  def num_stages(self):
    return len(self.stages)

  def size(self):
    return len(self.stages)

  def get_stage(self, index):
    return self.stages[index]
  def __getitem__(self, key):
      return self.stages.__getitem__(key)

  def to_str(self):
    return ("Battle {" +
            "stages: " + str(self.stages) +
            ", is_end: " + str(self.is_end) +
            ", is_won: " + str(self.is_won) +
            "}")
  def __str__(self):
    return self.to_str()
  def __repr__(self):
    return self.to_str()


# Assumes one unit per side.
class Stage:
  # Ctor vars.
  state = None
  friendly_hp = None # Friendly HP in the stage
  enemy_hp = None # Enemy HP in the stage
  is_end = None
  is_won = None

  # Vars added later:
  inp = None # Input into the neural network.
  q = None # The Q generated by the network from the inp, which determined the action if we didn't explore.
  action = None # The action that was taken on the state.
  # The reward we attribute to taking the action on the input. Changes over time as we learn more about
  # the consequences of having taken that action on that input.
  reward = None

  def __init__(self, state):
    self.state = state

    # Derived values:
    self.friendly_hp = 0 if not state.friendly_units else state.friendly_units.values()[0].get_life()
    self.enemy_hp = 0 if not state.enemy_units else state.enemy_units.values()[0].get_life()
    self.is_end = self.friendly_hp == 0 or self.enemy_hp == 0
    if self.is_end:
      self.is_won = self.friendly_hp > 0
    self.reward = 0

  def to_str(self):
    return ("Stage {" +
            "inp: " + str(self.inp) +
            ", q: " + str(self.q) +
            ", action: " + str(self.action) +
            ", friendly_hp: " + str(self.friendly_hp) +
            ", enemy_hp: " + str(self.enemy_hp) +
            ", is_end: " + str(self.is_end) +
            ", is_won: " + str(self.is_won) +
            "}")
  def __str__(self):
    return self.to_str()
  def __repr__(self):
    return self.to_str()


class Bot:
  # Infra
  sess = None

  # Q-network config.
  # TF-class objects.
  tf_inp = None # Prob should rename input to state to be consistent with Bellman.
  tf_q = None
  tf_target_q = None
  tf_action = None
  tf_train = None

  # Learning params
  explore = None

  n = None
  v = True # verbose

  battles = []
  current_battle = None
  total_reward = None

  def __init__(self):
    self.setup_q_nn()
    self.sess = tf.InteractiveSession()
    self.sess.run(tf.global_variables_initializer())
    self.total_reward = 0
    self.n = 0
    self.explore = INITIAL_EXPLORE

  def setup_q_nn(self):
    self.tf_inp = tf.placeholder(tf.float32, [1, INP_SHAPE])

    # Options for the network topology:
    # - Simple matrix multiplication
    # - 1 layer RELU neurons
    # - 2 layer RELU neurons

    ## MATMUL
    # self.tf_q = tf.matmul(self.tf_inp,
    #                tf.Variable(tf.random_uniform([INP_SHAPE,OUT_SHAPE],0,W_INIT)))

    ## 1-layer RELU
    # self.tf_q = tf.nn.relu(tf.matmul(self.tf_inp,
    #                tf.Variable(tf.random_uniform([INP_SHAPE,OUT_SHAPE],0,W_INIT))))

    ## 2-layer RELU
    layer_1 = tf.nn.relu(tf.matmul(self.tf_inp,
                   tf.Variable(tf.random_uniform([INP_SHAPE,20],0,W_INIT))))
    self.tf_q = tf.nn.relu(tf.matmul(layer_1,
                   tf.Variable(tf.random_uniform([20,OUT_SHAPE],0,W_INIT))))

    # TODO: Consider adding multiple channels.

    self.tf_action = tf.argmax(self.tf_q,1)

    # Obtain the loss by taking the sum of squares
    # difference between the target and prediction Q values.
    self.tf_target_q = tf.placeholder(tf.float32, [1, OUT_SHAPE])
    loss = tf.reduce_sum(tf.square(self.tf_target_q - self.tf_q))
    self.tf_train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)

  def update_battle(self, stage):
    if self.current_battle is None or self.current_battle[-1].is_end:
      self.current_battle = Battle()
      self.battles.append(self.current_battle)
    self.current_battle.add_stage(stage)

  def get_commands(self, state):
    # Skip every second frame.
    # This is because marine attacks take 2 frames, and so it can finish an attack started in a prev frame.
    # Need to make the current order an input into the NN so it can learn to return order-0 (no new order)
    # if already performing a good attack.):
    self.n += 1
    if self.n % FRAMES_PER_ACTION == 1: return []

    stage = Stage(state)
    self.update_battle(stage)

    # We still do this even if at the end, so that we have next_q values for the terminal state.
    # Else figure out what action to take next.
    inp = Bot.state_to_input(state, self.current_battle)
    if V: print "inp = " + str(inp)
    if V: print "inp_len = " + str(len(inp[0]))

    # Choose an action by greedily (with e chance of random action) from the Q-network
    # We need all the Q vals for learning.
    action,q = self.sess.run([self.tf_action, self.tf_q],
                                   feed_dict={self.tf_inp:inp})
    action = action[0]
    q_val = q[0][action]
    print "q = " + str(q)
    print "action = " + str(action)
    print "q_val = " + str(q_val)
    # Sometimes get a random action instead of what the model tells us, to explore,
    # or if our action is shit.
    if q_val < 0.005:
      action = random.randint(0,OUT_SHAPE-1)
      print "NFI"
    elif random.random() < self.explore:
      action = random.randint(0,OUT_SHAPE-1)
      print "EXP"
      self.explore *= EXPLORE_FACTOR
    else:
      self.explore *= EXPLORE_FACTOR
      print "ACT"
    print "explore = " + str(self.explore)
    print "total_reward = " + str(self.total_reward)

    stage.inp = inp
    stage.q = q
    stage.action = action

    if self.current_battle.is_end:
      # Once the battle is over, train for it wholistically:
      self.train_battle(self.current_battle)

    # Outputs
    return Bot.output_to_command(action, state)

  def train_battle(self, battle):
    print "\nTRAIN BATTLE"

    # First calculate rewards.

    for i in range(0, battle.size()):
      battle[i].reward = 0

    # ### 1. Instantaneous Reward
    # # action on i is rewarded for an improvement in advantage from i to i+1
    # for i in range(0, battle.size() - 1):
    #   adv = Bot.calculate_advantage(battle[i], battle[i+1])
    #   battle[i].reward += 0.1 if adv > 0 else 0

    # ### 2. Earn an advantage in 5 frames.
    # # action on i is rewarded for an improvement in advantage from i to i+5
    # for i in range(0, battle.size() - 5):
    #   adv = Bot.calculate_advantage(battle[i], battle[i+5])
    #   battle[i].reward = MINI_REWARD if adv > 0 else 0

    ### 2. Maintain an advantage for 5 frames.
    # action on i is rewarded for sustaining an advantage from i-5 to i
    #
    # So reward taking actions after earning an advantage that mean we don't lose it.
    # MT = 3
    # for i in range(MT, battle.size() - 1):
    #   initial_adv = Bot.calculate_advantage(battle[i-MT], battle[i-MT+1])
    #   final_adv = Bot.calculate_advantage(battle[i-MT], battle[i])
    #   battle[i].reward += 0.1 if (initial_adv > 0 and final_adv >= initial_adv) else 0
    MT = 9
    for i in range(10, battle.size() - 1):
      adv_0 = Bot.calculate_advantage(battle[i-10], battle[i-9])
      adv_3 = Bot.calculate_advantage(battle[i-10], battle[i-6])
      adv_6 = Bot.calculate_advantage(battle[i-10], battle[i-3])
      adv_9 = Bot.calculate_advantage(battle[i-10], battle[i])
      # if adv_0 > 0
      #   print "advs = " + str([adv_0, adv_3, adv_6, adv_9])
        # print "all = " + all(adv_k > adv_0 for adv_k in [adv_3, adv_6, adv_9])
      if adv_0 > 0 and all(adv_k >= adv_0 for adv_k in [adv_3, adv_6, adv_9]):
        battle[i].reward += 0.1
        battle[i-3].reward += 0.1
        battle[i-6].reward += 0.1
        battle[i-9].reward += 0.1
      # battle[i].reward += 0.1 if (initial_adv > 0 and final_adv >= initial_adv) else 0

    # HMM THIS makes things too cautious. No reward for taking risks.

    # Then train for all actions taken with given rewards.
    for i in range(0, battle.size() - 2):
      # stage = battle[i]
      print "battle[i] = " + str(battle[i])
      print "battle[i+1] = " + str(battle[i+1])
      self.total_reward += battle[i].reward
      self.train(battle[i].inp, battle[i].q, battle[i].action, battle[i+1].q, battle[i].reward)



  @staticmethod
  def calculate_advantage(stage_a, stage_b):
    """ What is the delata in advantage in moving from stage_a to stage_b """
    # Improvement in hp difference is good.
    a_hp_diff = stage_a.friendly_hp - stage_a.enemy_hp
    b_hp_diff = stage_b.friendly_hp - stage_b.enemy_hp
    return b_hp_diff - a_hp_diff

  def train(self, inp, q, action, next_q, reward):
    """ Train the network once to receive $reward for taking #action
        on $inp
    """
    # Train with loss from Bellman equation
    # if self.v:
    print "TRAIN"
    print "reward = " + str(reward)
    print "inp = " + str(inp)
    print "action = " + str(action)
    print "q = " + str(q)
    print "next_q = " + str(next_q)

    target_q = q
    target_q[0, action] = reward + GAMMA*np.max(next_q)
    print "target_q2 = " + str(target_q)
    # if all(r[0] == 0 for r in target_q):
    #   print "all zeros = " + str(target_q)
    #   sys.exit()

    if False:
      cur_v = target_q[0, action]
      if (reward == 1):
        new_v = reward
      else:
        new_v = max(0.001, GAMMA*min(1, np.max(next_q)))
      for i in range(0, len(target_q[0])):
        target_q[0, i] = max(0.001,min(target_q[0, i], GAMMA))
      # new_v = reward + GAMMA*np.max(next_q)
      print "cur_v = " + str(cur_v) + ", new_v = " + str(new_v)
      print "delta = " + str(cur_v - new_v) + ", pc = " + str(new_v / cur_v)
      # TODO: should there be a hyperparam learning rate here?
      target_q[0, action] = reward + GAMMA*np.max(next_q)
      target_q[0, action] = new_v
      print "target_q2 = " + str(target_q)
      if math.isnan(q[0,0]): sys.exit()
      if new_v > 100: sys.exit()
    # print "target_q = " + str(target_q)

    # By passing inp the network will regenerate prev_q, and then apply loss where
    # it differs from target_q - to train it to remember target_q instead of prev_q
    self.sess.run(self.tf_train,
                  feed_dict={self.tf_inp:inp,
                             self.tf_target_q:target_q})

  @staticmethod
  def output_to_command(action, state):
    """ out_t = [14] """
    """ action in [0 .. 13]"""


    friendly_units = state.friendly_units.values()
    enemy_units = state.enemy_units.values()


    commands = []
    # for i in range(0, max(len(friendly_units), 5)):
    for i in range(0, len(friendly_units)):
      if i == 5: break
      friendly = friendly_units[i]
      # one-hot
      # 0 = do nothing
      # 1-8 = move 5 units in dir
      # 9-13 = attack unit num 0-4
      a = action
      # a = np.argmax(y[i])
      if a == 0: continue # No new order, so prob continue with current order.
      elif 1 <= a and a <= 8:
        del_x, del_y = MOVES[a-1]
        move_x = Bot.constrain(friendly.x + del_x, X_MOVE_RANGE)
        move_y = Bot.constrain(friendly.y + del_y, Y_MOVE_RANGE)
        commands.append([friendly.id, tc_client.UNIT_CMD.Move, -1, move_x, move_y])
      elif 9 <= a and a <= OUT_SHAPE - 1:
        e_index = a - 9
        if e_index >= len(enemy_units): continue # Do nothing, can't attack unit that doesn't exist.
        commands.append([friendly.id, tc_client.UNIT_CMD.Attack_Unit, enemy_units[e_index].id])
      else:
        raise Exception("Invalid action: " + str(a))
    return commands


  @staticmethod
  def state_to_input(state, battle):
    """ returns [1,30] """
    # """ returns [10,3] """
    #max 10 units
    # First 5 inputs are friendly units
    # second 5 are enemy units
    friendly_units = state.friendly_units.values()
    enemy_units = state.enemy_units.values()
    if len(friendly_units) > MAX_FRIENDLY_UNITS: friendly_units = friendly_units[:MAX_FRIENDLY_UNITS]
    if len(enemy_units) > MAX_ENEMY_UNITS: enemy_units = enemy_units[:MAX_ENEMY_UNITS]

    friendly_tensor = Bot.pack_unit_tensor(
        Bot.units_to_tensor(friendly_units, True), FRIENDLY_TENSOR_SIZE, MAX_FRIENDLY_UNITS)


    enemy_tensor = Bot.pack_unit_tensor(
        Bot.units_to_tensor(enemy_units, False), ENEMY_TENSOR_SIZE, MAX_ENEMY_UNITS)

    extra = [[0,0]]
    if battle.size() >= 2:

      # print "battle = " + str(battle)
      # print "battle[-2] = " + str(battle[-2])
      # print "state.enemy_units = " + str(battle[-2].state.enemy_units.values())
      prev_foes = battle[-2].state.enemy_units.values()
      if len(prev_foes) >= 1:
        prev_enemy = prev_foes[0]
        extra = [[
              Bot.norm(prev_enemy.x, (X_MOVE_RANGE)),
              Bot.norm(prev_enemy.y, (Y_MOVE_RANGE))]]


    # ts = friendly_tensor + enemy_tensor
    # return
    # return [ [x] for x in itertools.chain.from_iterable(friendly_tensor + enemy_tensor)]
    return [list(itertools.chain.from_iterable(friendly_tensor + enemy_tensor + extra))]


  @staticmethod
  def units_to_tensor(units, is_friendly):
    return [Bot.unit_to_vector(unit, is_friendly) for unit in units]


  @staticmethod
  def unit_to_vector(unit, is_friendly):
    max_hp = 40


    unit_vector = [
            # 1.0 if is_friendly else -1.0,
            Bot.norm(unit.x, (X_MOVE_RANGE)),
            Bot.norm(unit.y, (Y_MOVE_RANGE)),
            float(unit.get_life()) / unit.get_max_life()
            ]

    print "unit = " + str(unit)
    if is_friendly:

      is_guard = False
      is_move = False
      is_attack = False
      # Seee https://bwapi.github.io/namespace_b_w_a_p_i_1_1_orders_1_1_enum.html
      if len(unit.orders) > 0:
        order_type = unit.orders[0].type
        is_guard = order_type in [2,3] #
        is_move = order_type == 6  #
        is_attack = order_type == 10 #
      unit_vector = unit_vector + [int(is_guard), int(is_move), int(is_attack), float(unit.groundCD) / unit.maxCD]
      # unit_vector = unit_vector + [int(is_guard), int(is_move), int(is_attack)]

    return unit_vector

  @staticmethod
  def pack_unit_tensor(unit_tensor, tensor_shape, final_rows):
    return unit_tensor + [[0 for i in range(0, tensor_shape)] for i in range(len(unit_tensor), final_rows)]

  @staticmethod
  def norm(x, min_max):
    # Map to range [-1.0,1.0]
    # return (2.0*(x - min_max[0]) / (min_max[1] - min_max[0])) - 1
    # Map to range [0,1.0]
    return (float(x) - min_max[0]) / (min_max[1] - min_max[0])

  @staticmethod
  def constrain(x, min_max):
    # truncate X to fit in [x_min,x_max]
    return min(max(x,min_max[0]),min_max[1])


